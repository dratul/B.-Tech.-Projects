\chapter{Important Codes/Analysis(1)}
\section{Jupyter Coding}
\lstset{language=Python}
\lstset{frame=lines}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
import numpy as np
import pandas as pd from matplotlib 
import pyplot as plt from sklearn.model_selection 
import train_test_split from sklearn.naive_bayes 
import GaussianNB, MultinomialNB from sklearn.metrics 
import accuracy_score, confusion_matrix, classification_report 
from sklearn.feature_extraction.text 
import TfidfVectorizer from sklearn.pipeline 
import make_pipeline
import string
import nltk
import re
import seaborn as sns

dataset_columns = ["target", "ids", "date", "flag", "user", "text"]
dataset_encode = "ISO-8859-1"

df = pd.read_csv("training.1600000.processed.noemoticon.csv",
encoding=dataset_encode, names=dataset_columns)
df.head()

df.drop(['ids', 'date', 'flag', 'user'], axis=1, inplace=True)
df['target'].value_counts()

# Remove punctuation
def remove_punctuation(text):
    no_punct = [words for words in text if words not in string.punctuation]
    words_wo_punct = ''.join(no_punct)
    return words_wo_punct

df['clean_text'] = df['text'].apply(lambda x: remove_punctuation(x))
df.head()

# Remove hyperlink
df['clean_text'] = df['clean_text'].str.replace(r"http\S+", "")

# Remove emoji
df['clean_text'] = df['clean_text'].str.replace('[^\w\s#@/:%.,_-]', '',
flags=re.UNICODE)

# Convert all words to lowercase
df['clean_text'] = df['clean_text'].str.lower()
df.head()

# Tokenization
nltk.download('punkt')

def tokenize(text):
    split = re.split("\W+", text)
    return split

df['clean_text_tokenize'] = df['clean_text'].apply(lambda x: tokenize(x.lower()))

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stopword = nltk.corpus.stopwords.words('english')

def remove_stopwords(text):
    text = [word for word in text if word not in stopword]
    return text

df['clean_text_tokenize_stopwords'] = 
df['clean_text_tokenize'].apply(lambda x: remove_stopwords(x))
df.head(10)

new_df = pd.DataFrame()
new_df['text'] = df['clean_text']
new_df['label'] = df['target']
new_df['label'] = new_df['label'].replace(4, 1)
print(new_df.head())
print('Label: \n', new_df['label'].value_counts())

from sklearn.model_selection import train_test_split

X = new_df['text']
y = new_df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05,
random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
y_train.value_counts()

model = make_pipeline(TfidfVectorizer(), MultinomialNB())
model.fit(X_train, y_train)

validation = model.predict(X_test)
validation1 = model.predict(X_train)

from sklearn.metrics import accuracy_score

accuracy_score(y_train, validation1)

from sklearn.metrics import accuracy_score

accuracy_score(y_test, validation)

cf_matrix = confusion_matrix(y_test, validation)
sns.heatmap(cf_matrix / np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Greens')
print(classification_report(y_test, validation))

train = pd.DataFrame()
train['label'] = y_train
train['text'] = X_train

def predict_category(s, train=X_train, model=model):
    pred = model.predict([s])
    return pred[0]

predict_category("i wanna shot myself")
predict_category("i love you")


\end{lstlisting}
